{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers as tfl\n",
    "from keras import Sequential\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import get_file\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>isFakeNews</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>True</td>\n",
       "      <td>fake-and-real-news-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>True</td>\n",
       "      <td>fake-and-real-news-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>True</td>\n",
       "      <td>fake-and-real-news-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>True</td>\n",
       "      <td>fake-and-real-news-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>True</td>\n",
       "      <td>fake-and-real-news-dataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  isFakeNews  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...        True   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...        True   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...        True   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...        True   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...        True   \n",
       "\n",
       "                          src  \n",
       "0  fake-and-real-news-dataset  \n",
       "1  fake-and-real-news-dataset  \n",
       "2  fake-and-real-news-dataset  \n",
       "3  fake-and-real-news-dataset  \n",
       "4  fake-and-real-news-dataset  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "dataset_dir = \"data/compiledData.csv\"\n",
    "df = pd.read_csv(dataset_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 42\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = df['title'].apply(lambda x: len(x.split())).max()\n",
    "print(\"Max sequence length:\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134694"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_len = len(df)\n",
    "dataset_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_validation_split(df):\n",
    "    X = df['title']\n",
    "    y = df['isFakeNews']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_validation_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(text):\n",
    "    import tensorflow as tf\n",
    "    punctuation = \"!\"\n",
    "    stopwords = {'whom', 'all', 'shouldn', 'wouldn', 'how', 's', 'they', 'were', 'mustn', 'after', 'who', 'its', 'our', 't', 'a', 'very', 'an', 'do', 'be', 'to', 'can', 'had', 'i', 'these', 'himself', 'up', 'just', 'them', 'now', 'has', 'too', 'below', 'did', 'shan', 'until', 'during', 'him', 'into', 'have', \"you'd\", 'haven', 'theirs', 'ourselves', 'once', \"isn't\", 'than', \"it's\", 'wasn', 'yours', \"mightn't\", 'here', 'ours', 'her', 'doing', 'd', 'yourself', 'y', 'before', 'does', 'then', 'between', 'some', 'with', \"needn't\", 'further', 'she', 'down', 'on', \"you'll\", 'for', 'other', 'any', 'their', 'from', 'each', 'most', 'because', 'and', 'few', 'in', \"you've\", 'o', 'but', 'didn', \"shouldn't\", 'that', \"weren't\", 'which', 'or', \"hasn't\", 'own', 'about', 'what', \"aren't\", 'couldn', 'doesn', 'as', \"wouldn't\", 'hasn', 'no', 'm', 'hers', 'hadn', 'aren', 'while', 'will', \"don't\", \"shan't\", 'why', 'at', 'mightn', 'themselves', 'weren', \"that'll\", 'isn', 'only', 'the', 'been', \"couldn't\", 'don', 'should', 'same', 'both', 'where', 'was', 'me', 'through', \"hadn't\", 've', 'against', 'if', 'under', 'such', 'is', 'll', \"haven't\", 'ain', 're', \"didn't\", 'nor', 'not', 'being', 'are', 'your', 'over', 'off', 'having', 'by', \"won't\", 'myself', 'out', 'more', \"wasn't\", \"doesn't\", 'won', 'this', 'my', 'again', 'ma', 'his', 'when', 'you', 'there', 'herself', 'yourselves', 'itself', 'of', \"she's\", 'needn', 'we', \"mustn't\", 'above', \"you're\", 'so', 'it', \"should've\", 'am', 'he', 'those'}\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.regex_replace(text, \"<[^>]+>\", \"\") \n",
    "    text = tf.strings.regex_replace(text, '[%s]' % punctuation, \"\") \n",
    "    for stopword in stopwords:\n",
    "        text = tf.strings.regex_replace(text, r\"\\b%s\\b\" % stopword, \"\") \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(train_ds, max_words, max_seqlen, output_mode = \"int\", standardize = \"lower_and_strip_punctuation\"):\n",
    "    train_text = train_ds.to_list()\n",
    "    tokenizer = tfl.TextVectorization(\n",
    "        standardize=standardize,\n",
    "        max_tokens=max_words,\n",
    "        output_sequence_length=42,\n",
    "        output_mode=output_mode\n",
    "    )\n",
    "    tokenizer.adapt(train_text)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(X_train, max_words, max_sequence_length, standardize=text_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings_v1(url, output_file, embedding_file, embedding_dim, vocabulary, max_words, max_seqlen):\n",
    "    embedding_vecs = dict()\n",
    "    word_idx = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    file_dir = get_file(output_file, url)\n",
    "\n",
    "    with zipfile.ZipFile(file_dir, \"r\") as f:\n",
    "        f.extractall(\"content/\")\n",
    "\n",
    "    with open(embedding_file, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embedding_vec = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_vecs[word] = embedding_vec\n",
    "\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    \n",
    "    for word, idx in word_idx.items():\n",
    "        if idx < max_words:\n",
    "            embedding_vec = embedding_vecs.get(word)\n",
    "        if embedding_vec is not None:\n",
    "            embedding_matrix[idx] = embedding_vec\n",
    "    \n",
    "    embedding = tfl.Embedding(max_words, embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=False, input_length=max_seqlen, trainable=False)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
     ]
    }
   ],
   "source": [
    "vocabulary = tokenizer.get_vocabulary()\n",
    "embedding = load_pretrained_embeddings_v1(\"https://nlp.stanford.edu/data/glove.twitter.27B.zip\", \"content/glove.twitter.27B.zip\", \"content/glove.twitter.27B.100d.txt\", 100, vocabulary=vocabulary, max_words=max_words, max_seqlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recurrent_neural_network(embedding_layer, max_words, max_seqlen, optimizer='adam'):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            embedding_layer,\n",
    "            tfl.Bidirectional(tfl.LSTM(128, return_sequences=True, input_shape=(max_words, max_seqlen))),\n",
    "            tfl.Bidirectional(tfl.LSTM(128, return_sequences=False)),\n",
    "            tfl.Dropout(0.2),\n",
    "            tfl.Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 42, 100)           100000000 \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 42, 256)          234496    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100,628,993\n",
      "Trainable params: 628,993\n",
      "Non-trainable params: 100,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_recurrent_neural_network(embedding, max_words, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"tmp/checkpoints\" \n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path),\n",
    "    ReduceLROnPlateau(),\n",
    "    TerminateOnNaN(),\n",
    "    EarlyStopping(patience=2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_x, train_y, batch_size = 64, epochs=5, callbacks=callbacks):\n",
    "    train_x = tokenizer(train_x)\n",
    "    history = model.fit(x=train_x, y=train_y, validation_split=0.17644900953, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1474/1474 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9220"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1474/1474 [==============================] - 573s 386ms/step - loss: 0.1927 - accuracy: 0.9220 - val_loss: 0.1344 - val_accuracy: 0.9474 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model, X_train, y_train, batch_size=64, epochs=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/model_preliminary.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model(\"models/model_preliminary.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/632 [==============================] - 38s 58ms/step - loss: 0.1371 - accuracy: 0.9467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13706918060779572, 0.9467458724975586]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.evaluate(tokenizer(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the config and weights\n",
    "dill.dump({ 'config': tokenizer.get_config(),\n",
    "            'weights': tokenizer.get_weights(),\n",
    "            'text_preprocessor': text_preprocessor }\n",
    "            , open(\"tokenizers/tokenizer_preliminary.dill\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_disk = pickle.load(open(\"./tokenizers/tokenizer_preliminary.pkl\", \"rb\"))\n",
    "tokenizer = tfl.TextVectorization.from_config(from_disk['config'])\n",
    "tokenizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "tokenizer.set_weights(from_disk['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/model_preliminary.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/632 [==============================] - 15s 24ms/step - loss: 0.1385 - accuracy: 0.9461\n",
      "Test Loss: 0.13852912187576294\n",
      "Test Accuracy: 0.9461024403572083\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(tokenizer(X_test), y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/632 [==============================] - 16s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model.predict(tokenizer(X_test))\n",
    "y_pred = (y_pred_probs > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.94      0.94      0.94      9662\n",
      "        Fake       0.95      0.95      0.95     10543\n",
      "\n",
      "    accuracy                           0.95     20205\n",
      "   macro avg       0.95      0.95      0.95     20205\n",
      "weighted avg       0.95      0.95      0.95     20205\n",
      "\n",
      "[[ 9112   550]\n",
      " [  539 10004]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_test, y_pred, target_names=[\"Real\", \"Fake\"])\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
